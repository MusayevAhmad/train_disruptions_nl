{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Loading and Setup {#data-loading}\n",
        "\n",
        "Let's start by importing necessary libraries and loading our datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported successfully!\n",
            "Pandas version: 2.2.3\n",
            "Numpy version: 1.24.3\n",
            "Matplotlib version: 3.10.3\n",
            "Seaborn version: 0.13.2\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "from datetime import datetime, timedelta\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# Configure plotting\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configure pandas display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "print(f\"Pandas version: {pd.__version__}\")\n",
        "print(f\"Numpy version: {np.__version__}\")\n",
        "print(f\"Matplotlib version: {plt.matplotlib.__version__}\")\n",
        "print(f\"Seaborn version: {sns.__version__}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÇ Loading Dutch Train Disruption Data...\n",
            "\n",
            "Found 14 CSV files:\n",
            "  - disruptions-2011.csv\n",
            "  - disruptions-2012.csv\n",
            "  - disruptions-2013.csv\n",
            "  - disruptions-2014.csv\n",
            "  - disruptions-2015.csv\n",
            "  - disruptions-2016.csv\n",
            "  - disruptions-2017.csv\n",
            "  - disruptions-2018.csv\n",
            "  - disruptions-2019.csv\n",
            "  - disruptions-2020.csv\n",
            "  - disruptions-2021.csv\n",
            "  - disruptions-2022.csv\n",
            "  - disruptions-2023.csv\n",
            "  - disruptions-2024.csv\n",
            "\n",
            "Loading disruptions-2011.csv...\n",
            "  Loaded 1,846 rows\n",
            "\n",
            "Loading disruptions-2012.csv...\n",
            "  Loaded 2,074 rows\n",
            "\n",
            "Loading disruptions-2013.csv...\n",
            "  Loaded 2,312 rows\n",
            "\n",
            "Loading disruptions-2014.csv...\n",
            "  Loaded 2,484 rows\n",
            "\n",
            "Loading disruptions-2015.csv...\n",
            "  Loaded 2,947 rows\n",
            "\n",
            "Loading disruptions-2016.csv...\n",
            "  Loaded 3,031 rows\n",
            "\n",
            "Loading disruptions-2017.csv...\n",
            "  Loaded 4,085 rows\n",
            "\n",
            "Loading disruptions-2018.csv...\n",
            "  Loaded 5,190 rows\n",
            "\n",
            "Loading disruptions-2019.csv...\n",
            "  Loaded 5,940 rows\n",
            "\n",
            "Loading disruptions-2020.csv...\n",
            "  Loaded 4,450 rows\n",
            "\n",
            "Loading disruptions-2021.csv...\n",
            "  Loaded 4,874 rows\n",
            "\n",
            "Loading disruptions-2022.csv...\n",
            "  Loaded 5,499 rows\n",
            "\n",
            "Loading disruptions-2023.csv...\n",
            "  Loaded 5,168 rows\n",
            "\n",
            "Loading disruptions-2024.csv...\n",
            "  Loaded 5,964 rows\n",
            "\n",
            "‚úÖ Successfully combined 14 files\n",
            "üìä Total dataset: 55,864 rows, 16 columns\n",
            "\n",
            "üéØ Data loaded successfully!\n",
            "Date range: 2011 - 2024\n"
          ]
        }
      ],
      "source": [
        "# Function to load and combine all disruption datasets\n",
        "def load_all_disruption_data(datasets_path='datasets/'):\n",
        "    \"\"\"\n",
        "    Load all CSV files from the datasets directory and combine them into a single DataFrame.\n",
        "    \n",
        "    Returns:\n",
        "        pd.DataFrame: Combined dataset with all years\n",
        "    \"\"\"\n",
        "    datasets_dir = Path(datasets_path)\n",
        "    csv_files = sorted(datasets_dir.glob('disruptions-*.csv'))\n",
        "    \n",
        "    print(f\"Found {len(csv_files)} CSV files:\")\n",
        "    for file in csv_files:\n",
        "        print(f\"  - {file.name}\")\n",
        "    \n",
        "    all_data = []\n",
        "    total_rows = 0\n",
        "    \n",
        "    for file in csv_files:\n",
        "        print(f\"\\nLoading {file.name}...\")\n",
        "        try:\n",
        "            df = pd.read_csv(file)\n",
        "            \n",
        "            # Add metadata columns\n",
        "            year = file.name.split('-')[1].split('.')[0]\n",
        "            df['year'] = int(year)\n",
        "            df['source_file'] = file.name\n",
        "            \n",
        "            print(f\"  Loaded {len(df):,} rows\")\n",
        "            total_rows += len(df)\n",
        "            all_data.append(df)\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"  Error loading {file.name}: {e}\")\n",
        "    \n",
        "    # Combine all dataframes\n",
        "    if all_data:\n",
        "        combined_df = pd.concat(all_data, ignore_index=True)\n",
        "        print(f\"\\n‚úÖ Successfully combined {len(all_data)} files\")\n",
        "        print(f\"üìä Total dataset: {len(combined_df):,} rows, {len(combined_df.columns)} columns\")\n",
        "        return combined_df\n",
        "    else:\n",
        "        print(\"‚ùå No data loaded!\")\n",
        "        return None\n",
        "\n",
        "# Load the data\n",
        "print(\"üöÇ Loading Dutch Train Disruption Data...\\n\")\n",
        "df = load_all_disruption_data()\n",
        "\n",
        "if df is not None:\n",
        "    print(f\"\\nüéØ Data loaded successfully!\")\n",
        "    print(f\"Date range: {df['year'].min()} - {df['year'].max()}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Failed to load data!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß Preprocessing data...\n",
            "‚úÖ Preprocessing complete!\n",
            "üìä Added 11 new features\n",
            "\n",
            "üìà Final dataset shape: (55864, 27)\n"
          ]
        }
      ],
      "source": [
        "# Data preprocessing and cleaning\n",
        "def preprocess_data(df):\n",
        "    \"\"\"\n",
        "    Clean and preprocess the disruption data.\n",
        "    \n",
        "    Args:\n",
        "        df (pd.DataFrame): Raw disruption data\n",
        "        \n",
        "    Returns:\n",
        "        pd.DataFrame: Cleaned and processed data\n",
        "    \"\"\"\n",
        "    print(\"üîß Preprocessing data...\")\n",
        "    \n",
        "    # Make a copy to avoid modifying original\n",
        "    df_clean = df.copy()\n",
        "    \n",
        "    # Convert datetime columns\n",
        "    df_clean['start_time'] = pd.to_datetime(df_clean['start_time'])\n",
        "    df_clean['end_time'] = pd.to_datetime(df_clean['end_time'])\n",
        "    \n",
        "    # Extract time-based features\n",
        "    df_clean['start_date'] = df_clean['start_time'].dt.date\n",
        "    df_clean['start_hour'] = df_clean['start_time'].dt.hour\n",
        "    df_clean['start_month'] = df_clean['start_time'].dt.month\n",
        "    df_clean['start_weekday'] = df_clean['start_time'].dt.dayofweek  # 0=Monday\n",
        "    df_clean['start_weekday_name'] = df_clean['start_time'].dt.day_name()\n",
        "    df_clean['start_month_name'] = df_clean['start_time'].dt.month_name()\n",
        "    \n",
        "    # Add season classification\n",
        "    def get_season(month):\n",
        "        if month in [12, 1, 2]:\n",
        "            return 'Winter'\n",
        "        elif month in [3, 4, 5]:\n",
        "            return 'Spring'\n",
        "        elif month in [6, 7, 8]:\n",
        "            return 'Summer'\n",
        "        else:\n",
        "            return 'Autumn'\n",
        "    \n",
        "    df_clean['season'] = df_clean['start_month'].apply(get_season)\n",
        "    \n",
        "    # Add time period classification\n",
        "    def get_time_period(hour):\n",
        "        if 5 <= hour < 9:\n",
        "            return 'Morning Rush'\n",
        "        elif 9 <= hour < 17:\n",
        "            return 'Day Time'\n",
        "        elif 17 <= hour < 20:\n",
        "            return 'Evening Rush'\n",
        "        elif 20 <= hour < 24:\n",
        "            return 'Evening'\n",
        "        else:\n",
        "            return 'Night'\n",
        "    \n",
        "    df_clean['time_period'] = df_clean['start_hour'].apply(get_time_period)\n",
        "    \n",
        "    # Count affected stations\n",
        "    df_clean['station_count'] = df_clean['rdt_station_names'].fillna('').apply(\n",
        "        lambda x: len([s.strip() for s in x.split(',') if s.strip()])\n",
        "    )\n",
        "    \n",
        "    # Boolean for weekend\n",
        "    df_clean['is_weekend'] = df_clean['start_weekday'].isin([5, 6])\n",
        "    \n",
        "    # Duration categories\n",
        "    def categorize_duration(minutes):\n",
        "        if minutes <= 30:\n",
        "            return 'Short (‚â§30 min)'\n",
        "        elif minutes <= 120:\n",
        "            return 'Medium (30-120 min)'\n",
        "        elif minutes <= 360:\n",
        "            return 'Long (2-6 hours)'\n",
        "        else:\n",
        "            return 'Very Long (>6 hours)'\n",
        "    \n",
        "    df_clean['duration_category'] = df_clean['duration_minutes'].apply(categorize_duration)\n",
        "    \n",
        "    print(f\"‚úÖ Preprocessing complete!\")\n",
        "    print(f\"üìä Added {len(df_clean.columns) - len(df.columns)} new features\")\n",
        "    \n",
        "    return df_clean\n",
        "\n",
        "# Apply preprocessing\n",
        "if df is not None:\n",
        "    df_processed = preprocess_data(df)\n",
        "    print(f\"\\nüìà Final dataset shape: {df_processed.shape}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No data to preprocess!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Dataset Overview {#dataset-overview}\n",
        "\n",
        "Let's explore the basic characteristics of our combined dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìã DATASET OVERVIEW\n",
            "==================================================\n",
            "üìä Total Disruptions: 55,864\n",
            "üìÖ Date Range: 2011-01-03 23:22:50 to 2024-12-31 16:09:32\n",
            "üóìÔ∏è Years Covered: 14 years (2011-2024)\n",
            "üìç Unique Routes: 661\n",
            "üöâ Unique Station Names: 1,975\n",
            "\n",
            "‚è±Ô∏è DURATION STATISTICS\n",
            "==================================================\n",
            "Average Duration: 159.2 minutes\n",
            "Median Duration: 62.0 minutes\n",
            "Shortest Disruption: 0.0 minutes\n",
            "Longest Disruption: 64927.0 minutes\n",
            "\n",
            "üìà YEARLY BREAKDOWN\n",
            "==================================================\n",
            "2011: 1,846 disruptions\n",
            "2012: 2,074 disruptions\n",
            "2013: 2,312 disruptions\n",
            "2014: 2,484 disruptions\n",
            "2015: 2,947 disruptions\n",
            "2016: 3,031 disruptions\n",
            "2017: 4,085 disruptions\n",
            "2018: 5,190 disruptions\n",
            "2019: 5,940 disruptions\n",
            "2020: 4,450 disruptions\n",
            "2021: 4,874 disruptions\n",
            "2022: 5,499 disruptions\n",
            "2023: 5,168 disruptions\n",
            "2024: 5,964 disruptions\n",
            "\n",
            "üéØ CAUSE GROUPS\n",
            "==================================================\n",
            "rolling stock: 20,111 (36.0%)\n",
            "infrastructure: 15,527 (27.8%)\n",
            "accidents: 6,190 (11.1%)\n",
            "external: 5,965 (10.7%)\n",
            "engineering work: 2,716 (4.9%)\n",
            "logistical: 2,403 (4.3%)\n",
            "unknown: 1,164 (2.1%)\n",
            "staff: 991 (1.8%)\n",
            "weather: 795 (1.4%)\n",
            "\n",
            "üìã DATASET INFO\n",
            "==================================================\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 55864 entries, 0 to 55863\n",
            "Data columns (total 27 columns):\n",
            " #   Column                Non-Null Count  Dtype         \n",
            "---  ------                --------------  -----         \n",
            " 0   rdt_id                55864 non-null  int64         \n",
            " 1   ns_lines              55864 non-null  object        \n",
            " 2   rdt_lines             54690 non-null  object        \n",
            " 3   rdt_lines_id          54690 non-null  object        \n",
            " 4   rdt_station_names     41162 non-null  object        \n",
            " 5   rdt_station_codes     41162 non-null  object        \n",
            " 6   cause_nl              55679 non-null  object        \n",
            " 7   cause_en              55679 non-null  object        \n",
            " 8   statistical_cause_nl  55679 non-null  object        \n",
            " 9   statistical_cause_en  55679 non-null  object        \n",
            " 10  cause_group           55862 non-null  object        \n",
            " 11  start_time            55864 non-null  datetime64[ns]\n",
            " 12  end_time              55756 non-null  datetime64[ns]\n",
            " 13  duration_minutes      55755 non-null  float64       \n",
            " 14  year                  55864 non-null  int64         \n",
            " 15  source_file           55864 non-null  object        \n",
            " 16  start_date            55864 non-null  object        \n",
            " 17  start_hour            55864 non-null  int32         \n",
            " 18  start_month           55864 non-null  int32         \n",
            " 19  start_weekday         55864 non-null  int32         \n",
            " 20  start_weekday_name    55864 non-null  object        \n",
            " 21  start_month_name      55864 non-null  object        \n",
            " 22  season                55864 non-null  object        \n",
            " 23  time_period           55864 non-null  object        \n",
            " 24  station_count         55864 non-null  int64         \n",
            " 25  is_weekend            55864 non-null  bool          \n",
            " 26  duration_category     55864 non-null  object        \n",
            "dtypes: bool(1), datetime64[ns](2), float64(1), int32(3), int64(3), object(17)\n",
            "memory usage: 10.5+ MB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# Dataset basic information\n",
        "if df_processed is not None:\n",
        "    print(\"üìã DATASET OVERVIEW\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    print(f\"üìä Total Disruptions: {len(df_processed):,}\")\n",
        "    print(f\"üìÖ Date Range: {df_processed['start_time'].min()} to {df_processed['start_time'].max()}\")\n",
        "    print(f\"üóìÔ∏è Years Covered: {df_processed['year'].nunique()} years ({df_processed['year'].min()}-{df_processed['year'].max()})\")\n",
        "    print(f\"üìç Unique Routes: {df_processed['rdt_lines'].nunique():,}\")\n",
        "    print(f\"üöâ Unique Station Names: {df_processed['rdt_station_names'].nunique():,}\")\n",
        "    \n",
        "    print(f\"\\n‚è±Ô∏è DURATION STATISTICS\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"Average Duration: {df_processed['duration_minutes'].mean():.1f} minutes\")\n",
        "    print(f\"Median Duration: {df_processed['duration_minutes'].median():.1f} minutes\")\n",
        "    print(f\"Shortest Disruption: {df_processed['duration_minutes'].min():.1f} minutes\")\n",
        "    print(f\"Longest Disruption: {df_processed['duration_minutes'].max():.1f} minutes\")\n",
        "    \n",
        "    print(f\"\\nüìà YEARLY BREAKDOWN\")\n",
        "    print(\"=\" * 50)\n",
        "    yearly_counts = df_processed['year'].value_counts().sort_index()\n",
        "    for year, count in yearly_counts.items():\n",
        "        print(f\"{year}: {count:,} disruptions\")\n",
        "    \n",
        "    print(f\"\\nüéØ CAUSE GROUPS\")\n",
        "    print(\"=\" * 50)\n",
        "    cause_counts = df_processed['cause_group'].value_counts()\n",
        "    for cause, count in cause_counts.items():\n",
        "        percentage = (count / len(df_processed)) * 100\n",
        "        print(f\"{cause}: {count:,} ({percentage:.1f}%)\")\n",
        "        \n",
        "    print(f\"\\nüìã DATASET INFO\")\n",
        "    print(\"=\" * 50)\n",
        "    print(df_processed.info())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
